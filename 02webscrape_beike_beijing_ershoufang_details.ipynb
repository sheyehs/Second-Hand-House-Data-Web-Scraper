{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca98b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(1)  # 寻找元素的最长等待时间，而不是打开网页的最长等待时间。打开网页好像会自动等待全部加载完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a067d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = pd.read_parquet('贝壳北京二手房房源条目信息.parquet')['网页']\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7f50f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "details = [] # 元素将是字典\n",
    "\n",
    "# 测试时检查普适性用\n",
    "random_urls = urls.copy()\n",
    "random.shuffle(random_urls)\n",
    "# 测试用\n",
    "selected_urls = ['https://bj.ke.com/ershoufang/101111550757.html']\n",
    "\n",
    "for url in tqdm(urls):  # for url in tqdm(urls):\n",
    "    driver.get(url)\n",
    "    d = {}\n",
    "    \n",
    "    # 1.编号与网页\n",
    "    d['编号'] = url[url.rfind('/')+1:url.rfind('.')]\n",
    "    d['网页'] = url\n",
    "\n",
    "    #start = time()\n",
    "    \n",
    "    # 2.标题。兼用来判断页面是否过期\n",
    "    title = driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div[2]/div/div/div[1]/h1')\n",
    "    if not title:\n",
    "        continue\n",
    "    title = title[0].text\n",
    "    d['标题'] = title\n",
    "    #print(2,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 3.必要信息\n",
    "    total_price = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[2]/div/span[1]').text\n",
    "    unit1 = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[2]/div/span[2]/span').text\n",
    "    d['总价'] = ' '.join([total_price, unit1])\n",
    "    unit_price = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[2]/div/div[1]/div[1]/span').text\n",
    "    unit2 = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[2]/div/div[1]/div[1]/i').text\n",
    "    d['均价'] = ' '.join([unit_price, unit2])\n",
    "    neighborhood = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[4]/div[1]/a[1]').text\n",
    "    d['小区'] = neighborhood\n",
    "    area = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[4]/div[2]/span[2]')\n",
    "    area = area.find_elements(By.XPATH,'a')\n",
    "    d['区域'] = ' '.join([a.text for a in area])\n",
    "    person = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[1]/div[2]/div[5]/div[2]/div/div[1]/div[2]/div[1]/a')\n",
    "    d['维护人'] = person.text\n",
    "    #print(3,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 网页主要部分，以下都有main作为搜寻出发点\n",
    "    main = driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div[1]')\n",
    "    \n",
    "    # 每一部分用for ... in ... 句式，来解决网页可能没有该方面信息而报错的问题\n",
    "    # 4.基本信息\n",
    "    for introduction in main.find_elements(By.ID,'introduction'):\n",
    "        base_attr = introduction.find_elements(By.XPATH,'div/div/div[1]/div[2]/ul/li')\n",
    "        for x in base_attr:\n",
    "            key = x.find_element(By.XPATH,'span').text\n",
    "            value = x.text.replace(key,'') \n",
    "            d['基本信息_'+key] = value\n",
    "            \n",
    "        transaction_attr = introduction.find_elements(By.XPATH,'div/div/div[2]/div[2]/ul/li')\n",
    "        for x in transaction_attr:\n",
    "            key = x.find_element(By.XPATH,'span').text\n",
    "            value = x.text.replace(key,'')\n",
    "            d['基本信息_'+key] = value\n",
    "    #print(4,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 5.房源特色\n",
    "    for feature in main.find_elements(By.CLASS_NAME,'introContent.showbasemore'):\n",
    "        for tag in feature.find_elements(By.CLASS_NAME,'tags.clear'):\n",
    "            key = '房源特色_'+tag.find_element(By.CLASS_NAME,'name').text\n",
    "            value = [x.text for x in tag.find_elements(By.XPATH,'div[2]/a')]\n",
    "            d[key] = value\n",
    "        for attr in feature.find_elements(By.CLASS_NAME,'baseattribute.clear'):\n",
    "            key = '房源特色_'+attr.find_element(By.XPATH,'div[1]').text\n",
    "            value = attr.find_element(By.XPATH,'div[2]').text\n",
    "            d[key] = value\n",
    "    #print(5,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 6.房主自荐\n",
    "    for owner in main.find_elements(By.XPATH,'div/div[@id=\"yezhuSell\"]'):\n",
    "        recommend = {}\n",
    "        for paragraph in owner.find_element(By.CLASS_NAME,'txt').find_elements(By.XPATH,'div'):\n",
    "            key = paragraph.find_element(By.XPATH,'b').text\n",
    "            value = paragraph.find_element(By.XPATH,'span').text\n",
    "            recommend[key] = value\n",
    "        d['房主自荐_'] = recommend\n",
    "    #print(6,time()-start)\n",
    "    #start = time()\n",
    "        \n",
    "    # 7.户型分间\n",
    "    for layout in main.find_elements(By.ID,'layout'):\n",
    "        d['户型分间_'] = [room.text for room in layout.find_elements(By.CLASS_NAME,'row')]\n",
    "    #print(7,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 8.小区简介\n",
    "    for neighborhood_card in main.find_elements(By.CLASS_NAME,'xiaoquCard'):\n",
    "        d['小区简介_详情网页'] = neighborhood_card.find_element(By.CLASS_NAME,'fr').get_attribute('href')\n",
    "        for neighborhood_info in neighborhood_card.find_element(By.CLASS_NAME,'xiaoqu_main.fl').find_elements(By.XPATH,'div'):\n",
    "            key = '小区简介_'+neighborhood_info.find_element(By.XPATH,'label').text\n",
    "            value = neighborhood_info.find_element(By.XPATH,'span').text\n",
    "            d[key] = value\n",
    "    #print(8,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 9.参考首付\n",
    "    for calculator in main.find_elements(By.ID,'calculator'):\n",
    "        calculator_parameter = calculator.find_element(By.CLASS_NAME,'item-top')\n",
    "        parameter_list = [parameter.text.replace('\\n',' ') for parameter in calculator_parameter.find_elements(By.XPATH,'dl')]\n",
    "        parameter_list = list(filter(None,parameter_list))  # 除去爬取过程中出现的空字符串\n",
    "        price = calculator_parameter.find_element(By.XPATH,'//*[@name=\"price\"]').get_attribute('value')\n",
    "        evaluation = calculator_parameter.find_element(By.XPATH,'//*[@name=\"evaluation\"]').get_attribute('value')\n",
    "        parameter_list[1] = parameter_list[1].replace(' ',f' {price} ')\n",
    "        parameter_list[2] = parameter_list[2].replace(' ',f' {evaluation} ')\n",
    "        d['参考首付_参数'] = parameter_list\n",
    "        calculator_result = calculator.find_element(By.CLASS_NAME,'result-text')\n",
    "        d['参考首付_结果'] = [result.text.replace('\\n',' ') for result in calculator_result.find_elements(By.XPATH,'div')]\n",
    "    #print(9,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    # 10.带看记录。记录多页时点击下一页后就会丢失整个页面的引用，放到最后再做\n",
    "    # 问题在于网速，click()后页面未必及时更新，导致下文重新挂上的引用仍在旧页面上，即使设置了强制暂停1秒\n",
    "    # 解决思路，带看反馈单独爬取，不干扰其他部分的信息\n",
    "    for visit in main.find_elements(By.XPATH,'//*[@class=\"daikan_content\"]'):\n",
    "        # visit在点击下一页后会刷新，存在引用丢失问题，而除“带看记录”外其他部分不会刷新。\n",
    "        #故异于其他部分，下文在刷新循环部分while中用main作查找元素的起始\n",
    "        feedbacks = {}\n",
    "        number_page = visit.find_element(By.CLASS_NAME,'daikanPager.clear').text.split(' ')[0].split('/')[-1]\n",
    "        number_page = int(number_page)\n",
    "        i = 1\n",
    "        while i <= number_page:\n",
    "            j = 1\n",
    "            for feedback in main.find_elements(By.CLASS_NAME,'daikan_item_content.fr.clear'):\n",
    "                #print(1)\n",
    "                agent_name = feedback.find_element(By.CLASS_NAME,'itemAgentName.LOGCLICK.CLICKDATA').text\n",
    "                agent_comment = feedback.find_element(By.CLASS_NAME,'des').text\n",
    "                feedbacks[agent_name] = agent_comment\n",
    "                if j == 1 :\n",
    "                    flag_name = agent_name\n",
    "                j += 1\n",
    "            i += 1\n",
    "            if i > number_page:\n",
    "                break\n",
    "            # 以下两步解决时而出现的ElementClickInterceptedException异常，来源百度\n",
    "            button = main.find_element(By.ID,'nextPageComment')\n",
    "            driver.execute_script('arguments[0].click();',button)\n",
    "\n",
    "            # 由于元素一直存在，只好通过文本内容有无变化来判断点击下一页后有无刷新。\n",
    "            # 第一位经纪人的姓名有无变化\n",
    "            locator = (By.CLASS_NAME,'itemAgentName.LOGCLICK.CLICKDATA')  # 定位第一位经纪人姓名的位置\n",
    "            WebDriverWait(main,10).until_not(EC.text_to_be_present_in_element(locator, flag_name))  # 直到不包含\n",
    "            #main = driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div[1]')  # main没有被刷新，不需重挂\n",
    "            #visit = main.find_element(By.XPATH,'//*[@id=\"daikanContainer\"]//*[@class=\"daikan_content\"]')\n",
    "\n",
    "        d['经纪人带看反馈'] = feedbacks\n",
    "    #print(10,time()-start)\n",
    "    #start = time()\n",
    "    \n",
    "    details.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2c84a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(details)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974c5f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e273eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.iloc[:,10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfebb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.iloc[:,20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bed405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.iloc[:,30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777d90b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.iloc[:,40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('贝壳北京二手房房源详细信息.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('贝壳北京二手房房源详细信息.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17678ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
